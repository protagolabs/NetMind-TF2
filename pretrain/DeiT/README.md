This repo is developing for pretraining [Training data-efficient image transformers & distillation through attention](https://arxiv.org/pdf/2012.12877.pdf)

The result on ImageNet-1k is:

|ImageNet-1k|top-1 | top-5|
|---        |---   |---   |
|           |--- |--- |

The pretrained model can be download from: TBD
