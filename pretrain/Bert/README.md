# Bert-TF2
In this repo we develop the pretraining for [Bert](https://arxiv.org/abs/1810.04805). We use the huggingface transofmers with Tensorflow 2.

We use wikipedia and bookcorpus for training this model and evaluate the results on GLUE.

## training ##

python train.py

## Acknowledgement ##
[Huggingface Transformers Language Modeling](https://github.com/huggingface/transformers/blob/master/examples/tensorflow/language-modeling/run_mlm.py)

Thanks for my colleages Xiangpeng Wan and Yu Cheng for their kindly helps
